{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing based on original annotation file - not very accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os import listdir, makedirs\n",
    "from os.path import join, exists\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import spacy\n",
    "import codecs\n",
    "import xmltodict\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_lg==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz#egg=en_core_web_lg==2.1.0 in /home/texuanw/softwares/anaconda3/lib/python3.6/site-packages (2.1.0)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "prefix = \"/home/texuanw/tools/allenNLP/\" # or = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/\"\n",
    "options_file = prefix + \"elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = prefix + \"elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = \"../Data/bioc_FH_training/\"\n",
    "PROCESSED_DIR = \"../Data/processed/\"\n",
    "OBJECTS_DIR = '../objects/'\n",
    "\n",
    "torch.set_num_threads(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pkl(path, file_name):\n",
    "    with open(join(path, file_name), 'rb') as pkl_f:\n",
    "        content = pkl.load(pkl_f)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_ent_type(token, entities):\n",
    "    # entity: [start, end, ent_type, content]\n",
    "    for entity in entities:\n",
    "#         if token.idx == entity[0]:\n",
    "#             return 'B-' + entity[2]\n",
    "#         elif token.idx > entity[0] and token.idx < entity[1]:\n",
    "#             return 'I-' + entity[2]\n",
    "        if token.idx >= entity[0] and token.idx < entity[1]:\n",
    "            if entity[2] == 'Observation':\n",
    "                return \"OBS\"\n",
    "            elif entity[2] == 'FamilyMember':\n",
    "                return \"FM\"\n",
    "            else:\n",
    "                print(entity[2])\n",
    "    return \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_labels(doc, entities, count):\n",
    "    \"\"\"\n",
    "    Get labels of all tokens in a document.\n",
    "    Each token is labeled as [entity_type, POS_type, dependency]\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_label = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        sent_label = []\n",
    "\n",
    "        for idx, token in enumerate(sent):\n",
    "            ent_type = get_token_ent_type(token, entities)\n",
    "\n",
    "#             if ent_type.startswith('B'):\n",
    "#                 print(token, ent_type)\n",
    "#                 count += 1\n",
    "\n",
    "            token.ent_type_ = ent_type\n",
    "\n",
    "            if not token.ent_type_ in entd.keys():\n",
    "                entd[token.ent_type_] = len(entd)\n",
    "\n",
    "            if not token.pos_ in posd.keys():\n",
    "                posd[token.pos_] = len(posd)\n",
    "\n",
    "            if not token.dep_ in depd.keys():\n",
    "                depd[token.dep_] = len(depd)\n",
    "\n",
    "            sent_label.append([entd[token.ent_type_],posd[token.pos_],depd[token.dep_]])\n",
    "\n",
    "        doc_label.append(sent_label)\n",
    "        \n",
    "    return doc_label, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "corpus_labels = {}\n",
    "\n",
    "entd = dict()\n",
    "posd = dict()\n",
    "depd = dict()\n",
    "entd['<PAD>'] = len(entd)\n",
    "posd['<PAD>'] = len(posd)\n",
    "depd['<PAD>'] = len(depd)\n",
    "\n",
    "texts = load_pkl(OBJECTS_DIR, 'texts.pkl')\n",
    "\n",
    "count = 0\n",
    "for title, doc in texts.items():\n",
    "    entities = load_pkl(OBJECTS_DIR, 'entities_corrected.pkl')\n",
    "    # print(entities[title]) ###\n",
    "    \n",
    "    doc = nlp(doc)\n",
    "    doc_labels, count = get_doc_labels(doc, entities[title], count)\n",
    "    corpus_labels[title] = doc_labels\n",
    "\n",
    "    # sents to a list of lists of tokens\n",
    "    doc_sent = list(map(lambda sent: list(map(lambda x: x.text, sent)), doc.sents))\n",
    "    corpus[title] = doc_sent\n",
    "    doc_emb = elmo(batch_to_ids(doc_sent)) # generate embemdding !! - list of lists [[embd_of_word, ], ]\n",
    "    assert doc_emb['mask'].sum().item() == len(doc)\n",
    "    \n",
    "    with open(join(PROCESSED_DIR, title + '.emb.pkl'), 'wb') as f:\n",
    "        pkl.dump(doc_emb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: [[[1, 3, 3], [1, 3, 3], [1, 3, 4], [1, 4, 5], [1, 4, 6], [1, 5, 7], [1, 1, 1], [1, 3, 8], [1, 1, 1], [1, 3, 9], [1, 6, 10], [1, 7, 11]], [[1, 3, 12], [1, 5, 7], [1, 1, 1], [1, 3, 3], [1, 3, 8], [1, 4, 6], [1, 5, 7], [1, 3, 8], [1, 5, 7], [1, 1, 1], [1, 8, 8], [1, 5, 7], [1, 8, 3], [1, 8, 8], [1, 6, 10], [1, 7, 11]], [[1, 2, 2], [1, 3, 12], [1, 4, 6], [1, 5, 27], [1, 4, 29], [1, 6, 10]], [[1, 8, 3], [1, 8, 12], [1, 4, 6], [1, 9, 14], [2, 3, 16], [1, 6, 10], [1, 3, 24], [1, 9, 14], [1, 6, 10], [1, 10, 12], [1, 4, 17], [3, 2, 2], [3, 3, 3], [3, 3, 16], [1, 6, 10], [1, 7, 11]], [[1, 10, 12], [1, 4, 6], [1, 11, 15], [1, 2, 32], [1, 13, 22], [1, 4, 23], [1, 9, 14], [1, 3, 16], [1, 6, 10], [1, 3, 24], [1, 9, 14], [1, 6, 10], [1, 10, 12], [1, 4, 17], [1, 11, 15], [1, 2, 32], [1, 6, 10], [1, 7, 11]], [[1, 1, 19], [2, 3, 12], [1, 6, 10], [1, 3, 24], [1, 9, 14], [1, 6, 10], [1, 4, 6], [1, 2, 32], [1, 13, 22], [1, 11, 23], [1, 6, 10], [1, 7, 11]], [[1, 1, 19], [2, 3, 12], [1, 6, 10], [1, 11, 15], [1, 3, 6], [1, 9, 14], [1, 6, 10], [1, 4, 41], [3, 3, 16], [1, 6, 10]], [[1, 8, 3], [1, 8, 12], [1, 4, 6], [1, 5, 27], [1, 1, 19], [1, 3, 4], [1, 6, 10], [1, 3, 24], [1, 9, 14], [1, 6, 10], [1, 4, 5], [1, 4, 28], [1, 13, 22], [1, 4, 23], [1, 1, 1], [1, 3, 16], [1, 4, 7], [1, 1, 19], [1, 2, 2], [1, 3, 8], [1, 6, 10], [1, 7, 11]], [[1, 5, 7], [1, 5, 21], [1, 1, 1], [1, 3, 8], [1, 5, 7], [1, 3, 8], [1, 6, 10], [1, 10, 12], [1, 4, 6], [1, 3, 16], [1, 13, 22], [1, 3, 23], [1, 5, 27], [1, 10, 12], [1, 4, 18], [1, 4, 29], [1, 2, 32], [1, 12, 18], [1, 4, 33], [1, 10, 16], [1, 5, 7], [1, 2, 2], [1, 3, 8], [1, 4, 7], [1, 3, 8], [1, 5, 7], [1, 1, 19], [1, 3, 8], [1, 6, 10]], [[1, 8, 3], [1, 8, 12], [1, 4, 6], [1, 5, 27], [1, 10, 12], [1, 4, 18], [1, 11, 34], [1, 4, 28], [1, 1, 1], [1, 3, 16], [1, 5, 7], [1, 13, 35], [1, 10, 8], [1, 13, 22], [1, 1, 19], [1, 3, 19], [1, 12, 20], [1, 3, 23], [1, 6, 10], [1, 7, 11]], [[1, 10, 12], [1, 4, 6], [1, 1, 41], [1, 8, 13], [1, 13, 22], [1, 5, 7], [1, 1, 1], [1, 2, 8], [1, 5, 7], [1, 1, 19], [1, 3, 8], [1, 11, 36], [1, 4, 23], [1, 1, 1], [1, 3, 13], [1, 5, 7], [1, 10, 8], [1, 6, 10]]]\n",
      "\n",
      "corpus: [['Family', 'history', 'information', 'was', 'obtained', 'from', 'the', 'patient', 'this', 'morning', '.', ' '], ['Details', 'from', 'the', 'family', 'history', 'are', 'on', 'file', 'in', 'the', 'Department', 'of', 'Medical', 'Genetics', '.', ' '], ['Pertinent', 'information', 'is', 'as', 'follows', ':'], ['Mrs.', 'Jacki', 'has', 'one', 'brother', ',', 'age', '40', ',', 'who', 'has', 'recurrent', 'thyroid', 'problems', '.', ' '], ['He', 'is', 'otherwise', 'healthy', 'and', 'has', 'one', 'daughter', ',', 'age', '8', ',', 'who', 'is', 'also', 'healthy', '.', ' '], ['Her', 'father', ',', 'age', '48', ',', 'is', 'alive', 'and', 'well', '.', ' '], ['Her', 'mother', ',', 'also', 'age', '66', ',', 'has', 'hypertension', '.'], ['Mrs.', 'Jacki', 'states', 'that', 'her', 'husband', ',', 'age', '43', ',', 'is', 'adopted', 'and', 'has', 'no', 'information', 'regarding', 'his', 'biological', 'family', '.', ' '], ['Due', 'to', 'this', 'lack', 'of', 'information', ',', 'they', 'desire', 'screening', 'and', 'testing', 'as', 'it', 'may', 'be', 'able', 'to', 'provide', 'them', 'with', 'more', 'information', 'regarding', 'risks', 'to', 'their', 'offspring', '.'], ['Mrs.', 'Jacki', 'states', 'that', 'she', 'does', 'not', 'know', 'the', 'countries', 'of', 'either', 'her', 'or', 'her', 'husband', \"'s\", 'ancestors', '.', ' '], ['They', 'are', 'both', 'Venezuela', 'and', 'to', 'the', 'best', 'of', 'their', 'knowledge', 'there', 'is', 'no', 'consanguinity', 'between', 'them', '.']]\n"
     ]
    }
   ],
   "source": [
    "print('corpus_labels:', corpus_labels[title])\n",
    "print('\\ncorpus:', corpus[title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(OBJECTS_DIR, 'corpus_labels.pkl'),'wb') as f:\n",
    "    pkl.dump(corpus_labels, f)\n",
    "    \n",
    "with open(join(OBJECTS_DIR, 'corpus.pkl'),'wb') as f:\n",
    "    pkl.dump(corpus, f)\n",
    "\n",
    "with open(join(OBJECTS_DIR, 'label_dict.pkl'),'wb') as f:\n",
    "    pkl.dump([entd, posd, depd], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [doc][sent][token]\n",
    "doc_emb['elmo_representations'][0][0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "{'<PAD>': 0, 'O': 1, 'FM': 2, 'OBS': 3}\n"
     ]
    }
   ],
   "source": [
    "print(len(entd))\n",
    "print(entd)\n",
    "# print(posd)\n",
    "# print(depd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
