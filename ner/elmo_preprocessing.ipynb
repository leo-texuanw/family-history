{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing based on original annotation file - not very accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os import listdir, makedirs\n",
    "from os.path import join, exists\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import spacy\n",
    "import codecs\n",
    "import xmltodict\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_lg==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz#egg=en_core_web_lg==2.1.0 in /home/texuanw/softwares/anaconda3/lib/python3.6/site-packages (2.1.0)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "prefix = \"/home/texuanw/tools/allenNLP/\" # or = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/\"\n",
    "options_file = prefix + \"elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = prefix + \"elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = \"../Data/bioc_FH_training/\"\n",
    "PROCESSED_DIR = \"../Data/processed/\"\n",
    "OBJECTS_DIR = '../objects/'\n",
    "\n",
    "torch.set_num_threads(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pkl(path, file_name):\n",
    "    with open(join(path, file_name), 'rb') as pkl_f:\n",
    "        content = pkl.load(pkl_f)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_ent_type(token, entities):\n",
    "    # entity: [start, end, ent_type, content]\n",
    "    for entity in entities:\n",
    "        if token.idx == entity[0]:\n",
    "            return 'B-' + entity[2]\n",
    "        elif token.idx > entity[0] and token.idx < entity[1]:\n",
    "            return 'I-' + entity[2]\n",
    "    return 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_labels(doc, entities, count):\n",
    "    \"\"\"\n",
    "    Get labels of all tokens in a document.\n",
    "    Each token is labeled as [entity_type, POS_type, dependency]\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_label = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        sent_label = []\n",
    "\n",
    "        for idx, token in enumerate(sent):\n",
    "            ent_type = get_token_ent_type(token, entities)\n",
    "\n",
    "            if ent_type.startswith('B'):\n",
    "                print(token, ent_type)\n",
    "                count += 1\n",
    "\n",
    "            token.ent_type_ = ent_type\n",
    "\n",
    "            if not token.ent_type_ in entd.keys():\n",
    "                entd[token.ent_type_] = len(entd)\n",
    "\n",
    "            if not token.pos_ in posd.keys():\n",
    "                posd[token.pos_] = len(posd)\n",
    "\n",
    "            if not token.dep_ in depd.keys():\n",
    "                depd[token.dep_] = len(depd)\n",
    "\n",
    "            sent_label.append([entd[token.ent_type_],posd[token.pos_],depd[token.dep_]])\n",
    "\n",
    "        doc_label.append(sent_label)\n",
    "        \n",
    "    return doc_label, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "entd = dict()\n",
    "posd = dict()\n",
    "depd = dict()\n",
    "entd['<PAD>'] = len(entd)\n",
    "posd['<PAD>'] = len(posd)\n",
    "depd['<PAD>'] = len(depd)\n",
    "\n",
    "texts = load_pkl(OBJECTS_DIR, 'texts.pkl')\n",
    "\n",
    "count = 0\n",
    "for title, doc in texts.items():\n",
    "    entities = load_pkl(OBJECTS_DIR, 'entities_corrected.pkl')\n",
    "    # print(entities[title]) ###\n",
    "    \n",
    "    doc = nlp(doc)\n",
    "    doc_labels, count = get_doc_labels(doc, entities[title], count)\n",
    "    labels.append(doc_labels)\n",
    "\n",
    "    # sents to a list of lists of tokens\n",
    "    doc_sent = list(map(lambda sent: list(map(lambda x: x.text, sent)), doc.sents))\n",
    "    corpus.append(doc_sent)\n",
    "    doc_emb = elmo(batch_to_ids(doc_sent)) # generate embemdding !! - list of lists [[embd_of_word, ], ]\n",
    "    assert doc_emb['mask'].sum().item() == len(doc)\n",
    "    \n",
    "    with open(join(PROCESSED_DIR, title + 'pkl'), 'wb') as f:\n",
    "        pkl.dump(doc_emb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1780\n"
     ]
    }
   ],
   "source": [
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: [[1, 1, 1], [1, 2, 2], [1, 2, 2], [1, 3, 3], [1, 3, 4], [1, 4, 5], [1, 4, 6], [1, 5, 7], [1, 1, 1], [1, 3, 8], [1, 5, 7], [1, 1, 1], [1, 3, 8], [1, 3, 9], [1, 6, 10], [1, 7, 11]]\n",
      "\n",
      "corpus: ['A', 'detailed', 'comprehensive', 'family', 'history', 'was', 'obtained', 'from', 'the', 'patient', 'during', 'the', 'visit', 'today', '.', ' ']\n"
     ]
    }
   ],
   "source": [
    "print('labels:', labels[0][0])\n",
    "print('\\ncorpus:', corpus[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(join(OBJECTS_DIR, 'labeled_corpus'), np.array(labels))\n",
    "\n",
    "with open(join(OBJECTS_DIR, 'label_dict.pkl'),'wb') as f:\n",
    "    pkl.dump([entd, posd, depd], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [doc][sent][token]\n",
    "doc_emb['elmo_representations'][0][0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "{'<PAD>': 0, 'O': 1, 'B-FamilyMember': 2, 'B-Observation': 3, 'I-Observation': 4, 'I-FamilyMember': 5}\n"
     ]
    }
   ],
   "source": [
    "print(len(entd))\n",
    "print(entd)\n",
    "# print(posd)\n",
    "# print(depd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
