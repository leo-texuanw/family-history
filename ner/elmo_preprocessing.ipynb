{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing based on original annotation file - not very accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from os import listdir, makedirs\n",
    "from os.path import join, exists\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import spacy\n",
    "import codecs\n",
    "import xmltodict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = \"../Data/bioc_FH_training/\"\n",
    "PROCESSED_DIR = \"../Data/processed/\"\n",
    "\n",
    "torch.set_num_threads(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml(path, f):\n",
    "    \"\"\"\n",
    "    Get Observations and FamaliMembers from xml file.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(join(path, f),'r') as f:\n",
    "        content = f.read()\n",
    "        content = xmltodict.parse(content)\n",
    "\n",
    "    Obs, Fam = [], []\n",
    "    for entity in content['data']['annotations']['entity']: # get `entity`s from xml\n",
    "        spans = []\n",
    "        for span in entity['span'].split(';'):\n",
    "            spans.append(span.split(','))\n",
    "\n",
    "        if entity['type'] == 'Observation':\n",
    "            Obs.extend(spans)\n",
    "        elif entity['type'] == 'FamilyMember':\n",
    "            Fam.extend(spans)\n",
    "\n",
    "    return {'OBS': Obs, 'FAM': Fam}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_lg==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz#egg=en_core_web_lg==2.1.0 in /home/texuanw/softwares/anaconda3/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "prefix = \"/home/texuanw/tools/allenNLP/\" # or = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/\"\n",
    "options_file = prefix + \"elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = prefix + \"elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_ent_type(token, entities):\n",
    "    \"\"\"\n",
    "    Get the entity type of a token.\n",
    "    \"\"\"\n",
    "\n",
    "    for type_, entities_ in entities.items():\n",
    "        for entity in entities_:\n",
    "            if token.idx >= int(entity[0]) and token.idx + len(token) <= int(entity[1]):\n",
    "                # if a token is in the span of a annotation with a type, then the token will be of that type\n",
    "                return type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_label(doc, entities):\n",
    "    \"\"\"\n",
    "    Get labels of all tokens in a document.\n",
    "    Each token is labeled as [entity_type, POS_type, dependency]\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_label = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        sent_label = []\n",
    "\n",
    "        for token in sent:\n",
    "            ent_type = get_token_ent_type(token, entities)\n",
    "            if ent_type:\n",
    "                token.ent_type_ = ent_type\n",
    "                print(token, ent_type)\n",
    "            else:\n",
    "                print(token, 'O')\n",
    "\n",
    "            if not token.ent_type_ in entd.keys():\n",
    "                entd[token.ent_type_] = len(entd)\n",
    "\n",
    "            if not token.pos_ in posd.keys():\n",
    "                posd[token.pos_] = len(posd)\n",
    "\n",
    "            if not token.dep_ in depd.keys():\n",
    "                depd[token.dep_] = len(depd)\n",
    "\n",
    "            sent_label.append([entd[token.ent_type_],posd[token.pos_],depd[token.dep_]])\n",
    "\n",
    "        doc_label.append(sent_label)\n",
    "        \n",
    "    return doc_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtfiles = [f for f in listdir(WORK_DIR) if f.endswith('.txt')]\n",
    "xmlfiles = [f[:-3]+'xml' for f in txtfiles]\n",
    "\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "entd = dict()\n",
    "posd = dict()\n",
    "depd = dict()\n",
    "entd['<PAD>'] = len(entd)\n",
    "posd['<PAD>'] = len(posd)\n",
    "depd['<PAD>'] = len(depd)\n",
    "\n",
    "for findex, txtfile in enumerate(txtfiles):\n",
    "    \n",
    "    entities = read_xml(WORK_DIR, xmlfiles[findex])\n",
    "    #print(entities) ###\n",
    "    \n",
    "    print(txtfile)\n",
    "    with open(join(WORK_DIR, txtfile),'r') as f:\n",
    "        content = f.read()\n",
    "        doc = nlp(content)\n",
    "\n",
    "    labels.append(get_doc_label(doc, entities))\n",
    "\n",
    "    doc_sent = list(map(lambda sent: list(map(lambda x: x.text, sent)), doc.sents)) # sents to a list of lists of tokens\n",
    "    corpus.append(doc_sent)\n",
    "    doc_emb = elmo(batch_to_ids(doc_sent)) # generate embemdding !! - list of lists [[embd_of_word, ], ]\n",
    "    assert doc_emb['mask'].sum().item() == len(doc)\n",
    "    with open(join(PROCESSED_DIR, txtfile[:-3] + 'pkl'), 'wb') as f:\n",
    "        pickle.dump(doc_emb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: [[1, 1, 1], [1, 2, 2], [1, 2, 2], [1, 3, 3], [1, 3, 4], [1, 4, 5], [1, 4, 6], [1, 5, 7], [1, 1, 1], [1, 3, 8], [1, 5, 7], [1, 1, 1], [1, 3, 8], [2, 3, 9], [1, 6, 10], [1, 7, 11]]\n",
      "\n",
      "corpus: ['A', 'detailed', 'comprehensive', 'family', 'history', 'was', 'obtained', 'from', 'the', 'patient', 'during', 'the', 'visit', 'today', '.', ' ']\n"
     ]
    }
   ],
   "source": [
    "print('labels:', labels[0][0])\n",
    "print('\\ncorpus:', corpus[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJECTS_DIR = '../objects/'\n",
    "np.save(join(OBJECTS_DIR, 'labeled_corpus'), np.array(labels))\n",
    "with open(join(OBJECTS_DIR, 'label_dict.pkl'),'wb') as f:\n",
    "    pickle.dump([entd, posd, depd], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_emb['elmo_representations'][0][0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "{'<PAD>': 0, '': 1, 'DATE': 2, 'PERSON': 3, 'FAM': 4, 'OBS': 5, 'CARDINAL': 6, 'GPE': 7, 'ORG': 8, 'QUANTITY': 9, 'PERCENT': 10, 'ORDINAL': 11, 'TIME': 12, 'NORP': 13, 'LOC': 14, 'LAW': 15, 'PRODUCT': 16}\n"
     ]
    }
   ],
   "source": [
    "print(len(entd))\n",
    "print(entd)\n",
    "# print(posd)\n",
    "# print(depd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
