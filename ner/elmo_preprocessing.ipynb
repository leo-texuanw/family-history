{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing based on original annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "import torch\n",
    "import spacy\n",
    "import pickle as pkl\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "\n",
    "\n",
    "WORK_DIR = \"../Data/bioc_FH_training/\"\n",
    "OBJECTS_DIR = '../objects/'\n",
    "PROCESSED_TRAIN_DATA_DIR = \"../Data/processed/\"\n",
    "PROCESSED_TEST_DATA_DIR = \"../Data/processed_test_data/\"\n",
    "\n",
    "torch.set_num_threads(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_lg==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz#egg=en_core_web_lg==2.1.0 in /home/texuanw/softwares/anaconda3/lib/python3.6/site-packages (2.1.0)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "prefix = \"/home/texuanw/tools/allenNLP/\" # or = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/\"\n",
    "options_file = prefix + \"elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = prefix + \"elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pkl(path, file_name):\n",
    "    with open(join(path, file_name), 'rb') as pkl_f:\n",
    "        content = pkl.load(pkl_f)\n",
    "    return content\n",
    "\n",
    "def pkl_dump(path, file_name, object_):\n",
    "    with open(join(path, file_name), 'wb') as pkl_file:\n",
    "        pkl.dump(object_, pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_ent_type(token, entities, bio=False):\n",
    "    \"\"\"\n",
    "        entity: [start, end, ent_type, content, attributes, id]\n",
    "    \"\"\"\n",
    "    \n",
    "    ret_type = \"O\"\n",
    "    \n",
    "    for entity in entities:\n",
    "        if token.idx >= entity[0] and token.idx < entity[1]:\n",
    "            if entity[2] == 'Observation':\n",
    "                ret_type = \"OBS\"\n",
    "                \n",
    "            elif entity[2] == 'FamilyMember':\n",
    "                if entity[4]['SideOfFamily'] in ['NA', None]:\n",
    "                    ret_type = \"FM\"\n",
    "                else:\n",
    "                    if str(token).strip().lower() not in [\"paternal\", \"maternal\"]:\n",
    "                        ret_type = entity[4]['SideOfFamily']  # Paternal or Maternal\n",
    "                    \n",
    "            elif entity[2] == 'LivingStatus':\n",
    "                alive, healthy = entity[4]['Alive'], entity[4]['Healthy']\n",
    "                \n",
    "                if 'No' in [alive, healthy]:\n",
    "                    ret_type = \"LS-0\"\n",
    "                elif alive == healthy == 'NA':\n",
    "                    ret_type = \"LS-1\"\n",
    "                elif alive == healthy == 'Yes':\n",
    "                    ret_type = \"LS-4\"\n",
    "                else:\n",
    "                    ret_type = \"LS-2\"    # one Yes, one NA\n",
    "\n",
    "            else:\n",
    "                print(entity[2])\n",
    "\n",
    "        prefix = \"\"\n",
    "        if bio:\n",
    "            if token.idx == entity[0]:\n",
    "                prefix = 'B-'\n",
    "            elif token.idx > entity[0] and token.idx < entity[1]:\n",
    "                prefix = 'I-'\n",
    "\n",
    "    return prefix + ret_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_labels(doc, entities, count, bio=False):\n",
    "    \"\"\"\n",
    "        Get labels of all tokens in a document.\n",
    "        Each token is labeled as [entity_type, POS_type, dependency]\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_label = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        sent_label = []\n",
    "\n",
    "        for idx, token in enumerate(sent):\n",
    "            ent_type = get_token_ent_type(token, entities)\n",
    "\n",
    "            if bio and ent_type.startswith('B'):\n",
    "                print(token, ent_type)\n",
    "                count += 1\n",
    "            elif ent_type != 'O':\n",
    "                count += 1\n",
    "\n",
    "            token.ent_type_ = ent_type\n",
    "            if not token.ent_type_ in entd.keys():\n",
    "                entd[token.ent_type_] = len(entd)\n",
    "\n",
    "            if not token.pos_ in posd.keys():\n",
    "                posd[token.pos_] = len(posd)\n",
    "\n",
    "            if not token.dep_ in depd.keys():\n",
    "                depd[token.dep_] = len(depd)\n",
    "\n",
    "            sent_label.append([entd[token.ent_type_],posd[token.pos_],depd[token.dep_]])\n",
    "\n",
    "        doc_label.append(sent_label)\n",
    "        \n",
    "    return doc_label, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entities in IO: 3164\n",
      "{'<PAD>': 0, 'FM': 1, 'O': 2, 'LS-2': 3, 'OBS': 4, 'LS-0': 5, 'Maternal': 6, 'Paternal': 7, 'LS-4': 8}\n"
     ]
    }
   ],
   "source": [
    "entd = dict()  # entity types dictionary\n",
    "posd = dict()  # part of speech dictionary\n",
    "depd = dict()  # dependency dictionary\n",
    "entd['<PAD>'] = len(entd)\n",
    "posd['<PAD>'] = len(posd)\n",
    "depd['<PAD>'] = len(depd)\n",
    "\n",
    "train_texts = load_pkl(OBJECTS_DIR, 'train_texts.pkl')\n",
    "entities = load_pkl(OBJECTS_DIR, 'entities_corrected.pkl')\n",
    "\n",
    "train_corpus_labels, count = {}, 0\n",
    "for title, doc in train_texts.items():\n",
    "    doc = nlp(doc)\n",
    "    doc_labels, count = get_doc_labels(doc, entities[title], count)\n",
    "    \n",
    "    train_corpus_labels[title] = doc_labels\n",
    "\n",
    "print(\"number of entities in IO:\", count)\n",
    "pkl_dump(OBJECTS_DIR, 'label_dict.pkl', [entd, posd, depd])\n",
    "print(entd)\n",
    "# print(posd)\n",
    "# print(depd)\n",
    "\n",
    "pkl_dump(OBJECTS_DIR, 'train_corpus_labels.pkl', train_corpus_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_corpus_labels:\n",
      " [[2, 10, 16], [2, 4, 4], [2, 6, 6], [2, 6, 17], [2, 7, 18], [2, 7, 1], [2, 5, 5], [2, 10, 16], [2, 6, 7], [2, 6, 15], [2, 2, 2], [2, 11, 19]]\n"
     ]
    }
   ],
   "source": [
    "print('train_corpus_labels:\\n', train_corpus_labels[title][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus:\n",
      " ['A', 'detailed', 'family', 'history', 'was', 'taken', 'during', 'the', 'visit', 'today', '.', ' ']\n"
     ]
    }
   ],
   "source": [
    "def embedding_docs(texts, path2store):\n",
    "    corpus = {}\n",
    "    \n",
    "    for title, doc in texts.items():\n",
    "        # sents to a list of lists of tokens\n",
    "        doc = nlp(doc)\n",
    "        doc_sent = list(map(lambda sent: list(map(lambda x: x.text, sent)), doc.sents))\n",
    "        corpus[title] = doc_sent\n",
    "\n",
    "        doc_emb = elmo(batch_to_ids(doc_sent)) # generate embemdding !! - list of lists [[embd_of_word, ], ]\n",
    "        assert doc_emb['mask'].sum().item() == len(doc)\n",
    "\n",
    "        # print(doc_emb['elmo_representations'][0][0][0].size()) # (batch_size, timesteps, embedding_dim)\n",
    "        pkl_dump(path2store, title + '.emb.pkl', doc_emb)\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "train_corpus = embedding_docs(train_texts, PROCESSED_TRAIN_DATA_DIR)\n",
    "pkl_dump(OBJECTS_DIR, 'train_corpus.pkl', train_corpus)\n",
    "print('corpus:\\n', train_corpus[title][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = load_pkl(OBJECTS_DIR, 'test_texts.pkl')\n",
    "test_corpus = embedding_docs(test_texts, PROCESSED_TEST_DATA_DIR)\n",
    "pkl_dump(OBJECTS_DIR, 'test_corpus.pkl', test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fake_doc_labels(doc):\n",
    "    \"\"\"\n",
    "        Get fake labels of all tokens in a test document.\n",
    "        Each token is labeled as [entity_type, POS_type, dependency] (3 dim)\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_label = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        sent_label = []\n",
    "\n",
    "        for idx, token in enumerate(sent):\n",
    "            sent_label.append([1, 1,1])\n",
    "\n",
    "        doc_label.append(sent_label)\n",
    "        \n",
    "    return doc_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus_labels = {}\n",
    "for title, doc in test_texts.items():\n",
    "    doc = nlp(doc)\n",
    "    doc_labels = get_fake_doc_labels(doc)\n",
    "    \n",
    "    test_corpus_labels[title] = doc_labels\n",
    "\n",
    "pkl_dump(OBJECTS_DIR, 'test_corpus_labels.pkl', test_corpus_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
