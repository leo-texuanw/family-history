{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, precision_score, recall_score, f1_score\n",
    "from tensorboardX import SummaryWriter\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import os\n",
    "import math\n",
    "from os import listdir, makedirs\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = '../Data/bioc_FH_training/'\n",
    "LOG_DIR  = '../log/bioc_FH_training/'\n",
    "\n",
    "lmap = lambda fun, it: list(map(lambda x: fun(x), it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WORK_DIR + 'dict.pkl','rb') as f:\n",
    "    ent, pos, dep = pickle.load(f)\n",
    "\n",
    "d_ent=len(ent)\n",
    "d_pos=len(pos)\n",
    "d_dep=len(dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_h):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W_q=nn.Linear(in_features=d_h,out_features=d_h,bias=True)\n",
    "        self.W_k=nn.Linear(in_features=d_h,out_features=d_h,bias=True)\n",
    "        self.W_v=nn.Linear(in_features=d_h,out_features=d_h,bias=True)\n",
    "        \n",
    "    def forward(self,q,k,v,mask):\n",
    "        Q=self.W_q(q)\n",
    "        K=self.W_k(k)\n",
    "        V=self.W_v(v)\n",
    "        O=torch.matmul(Q,torch.transpose(K,-2,-1))/ math.sqrt(Q.size(-1))\n",
    "        O=O.masked_fill(mask==0,-1e9)\n",
    "        E=F.softmax(O,dim=-1)\n",
    "        R=torch.matmul(E,v)\n",
    "#        G=F.sigmoid(self.W_g(R))*R\n",
    "        return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, d_x, d_h, d_ent, d_pos, d_dep, cell=nn.GRU, rnn_layers=3, ffn_layers=5, dp=0.5):\n",
    "        super(RNN, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dp)\n",
    "        #self.embedding = nn.Embedding(_weight=torch.tensor(emb,dtype=torch.float32),\n",
    "        #                              num_embeddings=embedding.shape[0],\n",
    "        #                              embedding_dim=embedding.shape[1])\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.d_h=d_h\n",
    "        self.rnn = cell(input_size=d_x, hidden_size=d_h, num_layers=rnn_layers, batch_first=True, dropout=dp,bidirectional=True)\n",
    "        self.ent_attn = Attention(d_h=d_h)\n",
    "        self.pos_attn = Attention(d_h=d_h)\n",
    "        self.dep_attn = Attention(d_h=d_h)\n",
    "        \n",
    "        self.hidden_ents = nn.ModuleList([nn.Linear(in_features=d_h, out_features=d_h) for _ in range(ffn_layers)])\n",
    "        self.hidden_poss = nn.ModuleList([nn.Linear(in_features=d_h, out_features=d_h) for _ in range(ffn_layers)])\n",
    "        self.hidden_deps = nn.ModuleList([nn.Linear(in_features=d_h, out_features=d_h) for _ in range(ffn_layers)])\n",
    "        \n",
    "        self.lm_ents = nn.ModuleList([nn.LayerNorm(d_h) for _ in range(ffn_layers)])\n",
    "        self.lm_poss = nn.ModuleList([nn.LayerNorm(d_h) for _ in range(ffn_layers)])\n",
    "        self.lm_deps = nn.ModuleList([nn.LayerNorm(d_h) for _ in range(ffn_layers)])\n",
    "        \n",
    "        self.f_ent_out = nn.Linear(in_features=d_h, out_features=d_ent)\n",
    "        self.f_pos_out = nn.Linear(in_features=d_h, out_features=d_pos)\n",
    "        self.f_dep_out = nn.Linear(in_features=d_h, out_features=d_dep)\n",
    "        \n",
    "        for p in self.parameters(): # initialize parameters\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, x, mask, hidden=None):\n",
    "        rnn_out, _ = self.rnn(x, hidden)\n",
    "        rnn_out = rnn_out[:, :, :self.d_h] + rnn_out[:, :, self.d_h:]\n",
    "        \n",
    "        ent_out_attn = self.ent_attn(q=rnn_out, k=rnn_out, v=rnn_out, mask=mask)\n",
    "        ent_out = ent_out_attn\n",
    "        for hl, lm in zip(self.hidden_ents, self.lm_ents):\n",
    "            ent_out = self.dropout(lm(F.leaky_relu(hl(ent_out))))\n",
    "        ent_out = self.f_ent_out(ent_out_attn + ent_out)\n",
    "        \n",
    "        pos_out_attn = self.pos_attn(q=rnn_out,k=rnn_out,v=rnn_out,mask=mask)\n",
    "        pos_out = pos_out_attn\n",
    "        for hl, lm in zip(self.hidden_poss, self.lm_poss):\n",
    "            pos_out = self.dropout(lm(F.leaky_relu(hl(pos_out))))\n",
    "        pos_out = self.f_pos_out(pos_out_attn + pos_out)\n",
    "        \n",
    "        dep_out_attn = self.dep_attn(q=rnn_out,k=rnn_out,v=rnn_out,mask=mask)\n",
    "        dep_out = dep_out_attn\n",
    "        for hl, lm in zip(self.hidden_deps, self.lm_deps):\n",
    "            dep_out = self.dropout(lm(F.leaky_relu(hl(dep_out))))\n",
    "        dep_out = self.f_dep_out(dep_out_attn + dep_out)\n",
    "        \n",
    "        return ent_out, pos_out, dep_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "    def __init__(self, model, dp=0.5,lr=1e-3):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "    \n",
    "    def train(self, X, mask, y_ent, y_pos, y_dep):\n",
    "        x = torch.tensor(X, dtype=torch.float)\n",
    "        attn_mask = (mask > 0).unsqueeze(1).repeat(1, mask.size(1), 1)\n",
    "        loss_mask = mask\n",
    "        y_true_ent = torch.tensor(y_ent, dtype=torch.long)\n",
    "        y_true_pos = torch.tensor(y_pos, dtype=torch.long)\n",
    "        y_true_dep = torch.tensor(y_dep, dtype=torch.long)\n",
    "        self.model.train(True)\n",
    "        self.optimizer.zero_grad()\n",
    "        y_hat_ent,y_hat_pos,y_hat_dep = self.model(x,attn_mask)\n",
    "        \n",
    "        loss_ent = self.maskNLLLoss(y_hat_ent, y_true_ent, loss_mask)\n",
    "        loss_pos = self.maskNLLLoss(y_hat_pos, y_true_pos, loss_mask)\n",
    "        loss_dep = self.maskNLLLoss(y_hat_dep, y_true_dep, loss_mask)\n",
    "        reg_lambda=1e-4\n",
    "        l2_reg = 0\n",
    "        \n",
    "        for W in self.model.named_parameters(): # regularizer\n",
    "            if \"weight\" in W[0]:\n",
    "                l2_reg = l2_reg + W[1].norm(2)\n",
    "        loss=loss_ent+loss_pos+loss_dep+l2_reg * reg_lambda\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(self.model.parameters(), 1) # gradient clip -> avoid gradient explosion\n",
    "        self.optimizer.step()\n",
    "        y_hat_ent = y_hat_ent.topk(1)[1].squeeze(-1).detach().cpu()\n",
    "        y_hat_pos = y_hat_pos.topk(1)[1].squeeze(-1).detach().cpu()\n",
    "        y_hat_dep = y_hat_dep.topk(1)[1].squeeze(-1).detach().cpu()\n",
    "        return loss_ent.item(), loss_pos.item(), loss_dep.item(), y_hat_ent.numpy(), y_hat_pos.numpy(), y_hat_dep.numpy()\n",
    "    \n",
    "    def test(self, X, mask, y_ent, y_pos, y_dep):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(X, dtype=torch.float)\n",
    "            attn_mask = (mask > 0).unsqueeze(1).repeat(1, mask.size(1), 1)\n",
    "            loss_mask = mask\n",
    "            y_true_ent = torch.tensor(y_ent, dtype=torch.long)\n",
    "            y_true_pos = torch.tensor(y_pos, dtype=torch.long)\n",
    "            y_true_dep = torch.tensor(y_dep, dtype=torch.long)\n",
    "            y_hat_ent,y_hat_pos,y_hat_dep = self.model(x,attn_mask)\n",
    "            \n",
    "            loss_ent = self.maskNLLLoss(y_hat_ent, y_true_ent, loss_mask)\n",
    "            loss_pos = self.maskNLLLoss(y_hat_pos, y_true_pos, loss_mask)\n",
    "            loss_dep = self.maskNLLLoss(y_hat_dep, y_true_dep, loss_mask)\n",
    "            \n",
    "            y_hat_ent = y_hat_ent.topk(1)[1].squeeze(-1).detach().cpu()\n",
    "            y_hat_pos = y_hat_pos.topk(1)[1].squeeze(-1).detach().cpu()\n",
    "            y_hat_dep = y_hat_dep.topk(1)[1].squeeze(-1).detach().cpu()\n",
    "            \n",
    "            return loss_ent.cpu().item(), loss_pos.cpu().item(), loss_dep.cpu().item(), y_hat_ent.numpy(), y_hat_pos.numpy(), y_hat_dep.numpy()\n",
    "    \n",
    "    def inference(self, X, mask):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.tensor(X, dtype=torch.float)\n",
    "            attn_mask = (mask > 0).unsqueeze(1).repeat(1, mask.size(1), 1)\n",
    "            y_hat_ent,y_hat_pos,y_hat_dep = self.model(x,attn_mask)\n",
    "            y_hat_ent = y_hat_ent.topk(1)[1].squeeze(-1).detach().cpu()\n",
    "            y_hat_pos = y_hat_pos.topk(1)[1].squeeze(-1).detach().cpu()\n",
    "            y_hat_dep = y_hat_dep.topk(1)[1].squeeze(-1).detach().cpu()\n",
    "            return y_hat_ent.numpy().flatten(), y_hat_pos.numpy(), y_hat_dep.numpy()\n",
    "    \n",
    "    def maskNLLLoss(self, logits, target, mask):\n",
    "        logits=F.softmax(logits,dim=-1)\n",
    "        crossEntropy = -torch.log(logits.gather(-1,target[:,:,None])).squeeze(-1)\n",
    "        loss = crossEntropy.masked_select(mask).mean()\n",
    "        return loss\n",
    "    \n",
    "    def load_model(self, model_path='./Model'):\n",
    "        self.model = torch.load(model_path + '/model.pkl')\n",
    "    \n",
    "    def save_model(self, model_path='./Model'):\n",
    "        if not os.path.exists(model_path):\n",
    "            os.mkdir(model_path)\n",
    "        torch.save(self.model, model_path + '/model.pkl')\n",
    "\n",
    "def build_conv(n_gram=[1,3,5],in_channels=[300,300,300], out_channels=[50,50,50]):\n",
    "    convs=nn.ModuleList([nn.Conv1d(in_channels=d_in,out_channels=d_out,kernel_size=n,padding=p,stride=1) for p,(d_in, d_out, n) in enumerate(zip(in_channels,out_channels,n_gram))])\n",
    "    return convs\n",
    "        \n",
    "def pad(sents,dim=5):\n",
    "    padded=[]\n",
    "    max_leng=max(lmap(lambda x:x.shape[0],sents))\n",
    "    for s in sents:\n",
    "        if s.shape[0]<=max_leng:\n",
    "            padded.append(np.concatenate((s,np.zeros((max_leng-s.shape[0],dim),dtype=np.int32))))\n",
    "    return np.array(padded)\n",
    "\n",
    "def unpad(y_true, y_hat):\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_hat = np.concatenate(y_hat)\n",
    "    idx = (y_true != 0)\n",
    "    y_true = y_true[idx]\n",
    "    y_hat = y_hat[idx]\n",
    "    return y_true, y_hat\n",
    "\n",
    "def mask_unpad(mask, y_hat):\n",
    "    y_hat = np.concatenate(y_hat)\n",
    "    idx = (mask != 0)\n",
    "    mask = mask[idx]\n",
    "    y_hat = y_hat[idx]\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RNN(d_x=1024,d_h=100,d_ent=d_ent,d_pos=d_pos,d_dep=d_dep,rnn_layers=1,ffn_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier(model=model,lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=LOG_DIR)\n",
    "train_size = 72\n",
    "\n",
    "txt_files = [f for f in listdir(WORK_DIR) if f.endswith('.txt')]\n",
    "\n",
    "\n",
    "emb_files = list(map(lambda x: x[:-3]+'pkl',txt_files))\n",
    "\n",
    "labels=np.load(WORK_DIR + 'label.npy')\n",
    "\n",
    "def read_emb_f(f):\n",
    "    with open(WORK_DIR + f, 'rb') as emb_f:\n",
    "        corpus = pickle.load(emb_f)\n",
    "    return corpus\n",
    "\n",
    "pool = Pool(processes=8) \n",
    "all_corpus = pool.map(read_emb_f,emb_files)\n",
    "\n",
    "all_corpus = list(zip(all_corpus,labels))\n",
    "train_corpus = all_corpus[:train_size]\n",
    "test_corpus = all_corpus[train_size:]\n",
    "\n",
    "#corpus = list(zip(emb_files,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in tqdm(range(20)): \n",
    "    np.random.shuffle(train_corpus)\n",
    "    for fi, f in enumerate(train_corpus):\n",
    "        \n",
    "        corpus = f[0]\n",
    "        label = lmap(lambda x: np.array(x), f[1])\n",
    "        label = pad(label,dim=3)\n",
    "        dl = []\n",
    "        i = 0\n",
    "        while i < label.shape[0]:\n",
    "            if label.shape[0]==0: break\n",
    "            x = corpus['elmo_representations'][0][i:i + batch_size]\n",
    "            mask =  corpus['mask'].type(torch.uint8)[i:i + batch_size]\n",
    "            y = label[i:i + batch_size]\n",
    "            l1,l2,l3,y1,y2,y3=clf.train(x,mask,y_ent=y[:,:,0],y_pos=y[:,:,1],y_dep=y[:,:,2])\n",
    "            i += batch_size\n",
    "            dl.append(l1+l2+l3)\n",
    "        writer.add_scalar('train loss',np.mean(dl),e*len(train_corpus)+fi)\n",
    "        #print(f'train epoch: {e}, doc: {fi}, loss: {np.mean(dl):.2f}')\n",
    "    print('-'*18)\n",
    "\n",
    "    tall, pall = [], []\n",
    "    for fi, f in enumerate(test_corpus):\n",
    "        corpus = f[0]\n",
    "        label = lmap(lambda x: np.array(x), f[1])\n",
    "        label = pad(label,dim=3)\n",
    "        pent = []\n",
    "        ppos = []\n",
    "        pdep = []\n",
    "        tent = []\n",
    "        tpos = []\n",
    "        tdep = []\n",
    "        dl = []\n",
    "        i=0\n",
    "        while i<label.shape[0]:\n",
    "            if label.shape[0]==0:break\n",
    "            x = corpus['elmo_representations'][0][i:i+batch_size]\n",
    "            mask =  corpus['mask'].type(torch.uint8)[i:i+batch_size]\n",
    "            y = label[i:i+batch_size]\n",
    "            l1,l2,l3,y1,y2,y3=clf.test(x,mask,y_ent=y[:,:,0],y_pos=y[:,:,1],y_dep=y[:,:,2])\n",
    "            i += batch_size\n",
    "            dl.append(l1+l2+l3)\n",
    "            pent.append(y1.flatten())\n",
    "            ppos.append(y2.flatten())\n",
    "            pdep.append(y3.flatten())\n",
    "            tent.append(y[:,:,0].flatten())\n",
    "            tpos.append(y[:,:,1].flatten())\n",
    "            tdep.append(y[:,:,2].flatten())\n",
    "        tent, pent = unpad(tent,pent)\n",
    "        tpos, ppos = unpad(tpos,ppos)\n",
    "        tdep, pdep = unpad(tdep,pdep)\n",
    "        \n",
    "        acc = accuracy_score(y_true=tent,y_pred=pent)+accuracy_score(y_true=tpos,y_pred=ppos)+accuracy_score(y_true=tdep,y_pred=pdep)\n",
    "        \n",
    "        tent = (tent==ent['OBS']) * tent / ent['OBS']\n",
    "        pent = (pent==ent['OBS']) * pent / ent['OBS']\n",
    "\n",
    "        tall.extend(tent)\n",
    "        pall.extend(pent)\n",
    "        lp = precision_score(y_true=tent, y_pred=pent)\n",
    "        lr = recall_score(y_true=tent, y_pred=pent)\n",
    "        lf1 = f1_score(y_true=tent, y_pred=pent)\n",
    "        \n",
    "        writer.add_scalar('test loss', np.mean(dl),e*len(test_corpus)+fi)\n",
    "        writer.add_scalar('test acc', acc,e*len(test_corpus)+fi)\n",
    "        writer.add_scalar('loc precision', np.mean(lp), e*len(test_corpus)+fi)\n",
    "        writer.add_scalar('loc recall',np.mean(lr),e*len(test_corpus)+fi)\n",
    "        writer.add_scalar('loc f1-score',np.mean(lf1),e*len(test_corpus)+fi)\n",
    "        \n",
    "        print(f'test epoch: {e}, doc: {fi}, loss: {np.mean(dl)}, acc: {acc}, precision: {lp:.2f}, recall: {lr:.2f}, f1-score: {lf1:.2f}')\n",
    "    print('-'*18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "print(len(test_corpus))\n",
    "for fi,f in enumerate(test_corpus):\n",
    "    corpus = f[0]\n",
    "    label = lmap(lambda x: np.array(x), f[1])\n",
    "    label = pad(label,dim=3)\n",
    "    pent = []\n",
    "    ppos = []\n",
    "    pdep = []\n",
    "    tent = []\n",
    "    tpos = []\n",
    "    tdep = []\n",
    "    i=0\n",
    "    while i<label.shape[0]:\n",
    "        if label.shape[0]==0:break\n",
    "        x = corpus['elmo_representations'][0][i:i+batch_size]\n",
    "        mask =  corpus['mask'].type(torch.uint8)[i:i+batch_size]\n",
    "        y = label[i:i+batch_size]\n",
    "        y1,y2,y3=clf.inference(x,mask)\n",
    "        i+=batch_size\n",
    "        pent.append(y1.flatten())\n",
    "        ppos.append(y2.flatten())\n",
    "        pdep.append(y3.flatten())\n",
    "        tent.append(y[:,:,0].flatten())\n",
    "        tpos.append(y[:,:,1].flatten())\n",
    "        tdep.append(y[:,:,2].flatten())\n",
    "    mask = corpus['mask'].numpy().flatten()\n",
    "    pent = mask_unpad(mask,pent)\n",
    "    ppos = mask_unpad(mask,ppos)\n",
    "    pdep = mask_unpad(mask,pdep)\n",
    "    pent = (pent==ent['OBS'])*pent/ent['OBS']\n",
    "    result.append(pent)\n",
    "np.save(WORK_DIR+'result', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import spacy\n",
    "import pickle\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from os import listdir, makedirs\n",
    "from os.path import join, exists\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = txt_files[train_size:]\n",
    "\n",
    "labels = np.load(WORK_DIR + 'result.npy')\n",
    "\n",
    "for findex, txt_file in enumerate(test_files):\n",
    "    with open(WORK_DIR + txt_file, 'r') as f:\n",
    "        doc = f.read()\n",
    "        doc = nlp(doc)\n",
    "    #print(len(doc))\n",
    "    label = labels[findex]\n",
    "    assert len(label) == len(doc)\n",
    "\n",
    "    for tid, token in enumerate(doc):\n",
    "        if label[tid] == 1:\n",
    "            print(token, 'OBS')\n",
    "        else:\n",
    "            print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = precision_score(y_true=tall, y_pred=pall)\n",
    "r = recall_score(y_true=tall, y_pred=pall)\n",
    "f1 = f1_score(y_true=tall, y_pred=pall)\n",
    "print(f'precision: {p:.2f}, recall: {r:.2f}, f1-score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
