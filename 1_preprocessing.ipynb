{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation errors hava found:\n",
    "\n",
    "## Minor\n",
    "- doc_50: full anned as FM\n",
    "- doc_157: daughte without r\n",
    "\n",
    "## Big issue\n",
    "- doc_169 !!\n",
    "- doc_59: 3 lost last\n",
    "- doc_66: 4 lost last\n",
    "- doc_4: 10+ 3 offset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "directory  = './Data/bioc-FH-training/'\n",
    "file_list  = listdir(directory)\n",
    "file_names = set([file.split('.')[0] for file in file_list if file != '' and file.endswith('txt')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "#import chardet # chardet.detect(binary_var)['encoding']\n",
    "\n",
    "texts = dict()\n",
    "\n",
    "for file_name in file_names:\n",
    "    file = directory + file_name + '.txt'\n",
    "    \n",
    "    with open(file, 'r') as f:\n",
    "        texts[file_name] = f.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "sents  = dict()\n",
    "tokens = dict()\n",
    "pos    = dict()\n",
    "\n",
    "for file_name in file_names:\n",
    "    tokens_   = []\n",
    "    tags      = []\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(texts[file_name])\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        tokens_.append(words)\n",
    "        tags.append(nltk.pos_tag(words))\n",
    "\n",
    "    sents[file_name] = sentences\n",
    "    tokens[file_name] = tokens_\n",
    "    pos[file_name] = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get annotated entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree\n",
    "\n",
    "entities = {}\n",
    "for file_name in file_names:\n",
    "    file_path = directory + file_name + '.xml'\n",
    "    \n",
    "    root = xml.etree.ElementTree.parse(file_path).getroot()\n",
    "    annotations = root.findall('annotations')[0]\n",
    "\n",
    "    entities_ = [] # get span of every annoted entity\n",
    "    for entity in annotations.findall('entity'):\n",
    "        typee = entity.find('type')\n",
    "\n",
    "        if typee.text in ['FamilyMember', 'Observation']: # other options: Age, LivingStatus, PHI\n",
    "            for spans in entity.findall('span')[0].text.split(';'):\n",
    "                span = list(map(int, spans.split(',')))\n",
    "                entities_.append((span[0], span[1], typee.text, texts[file_name][span[0]:span[1]]))\n",
    "\n",
    "    entities_.sort(key=lambda tup: tup[0])\n",
    "    entities[file_name] = entities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_entities = 0\n",
    "for file_name in file_names:\n",
    "    print('\\n', file_name)\n",
    "#     print(texts[file_name])\n",
    "    for entity in entities[file_name]:\n",
    "        num_entities += 1\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 1787 enties in all.\n"
     ]
    }
   ],
   "source": [
    "print(\"there are\", num_entities, \"enties in all.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store to pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join\n",
    "\n",
    "OBJECTS = './objects/'\n",
    "with open(join(OBJECTS, \"texts.pkl\"), 'wb') as pkl_file:\n",
    "    pickle.dump(texts, pkl_file)\n",
    "with open(join(OBJECTS, \"sents.pkl\"), 'wb') as pkl_file:\n",
    "    pickle.dump(sents, pkl_file)\n",
    "with open(join(OBJECTS, \"tokens.pkl\"), 'wb') as pkl_file:\n",
    "    pickle.dump(tokens, pkl_file)\n",
    "with open(join(OBJECTS, \"pos.pkl\"), 'wb') as pkl_file:\n",
    "    pickle.dump(pos, pkl_file)\n",
    "with open(join(OBJECTS, \"entities.pkl\"), 'wb') as pkl_file:\n",
    "    pickle.dump(entities, pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
