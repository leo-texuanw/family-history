{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation errors hava found:\n",
    "\n",
    "- doc_50: \"full\" anned as FM\n",
    "- doc_169: first char missing at idx 10\n",
    "- doc_59: 3 lost last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "OBJECTS = './objects/'\n",
    "train_data_directory = './Data/bioc-FH-training/'\n",
    "test_data_directory = './Data/testRelease-0805/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names(path):\n",
    "    file_list  = listdir(path)\n",
    "    return set([file.split('.')[0] for file in file_list if file != '' and file.endswith('txt')])\n",
    "\n",
    "train_file_names = get_file_names(train_data_directory)\n",
    "test_file_names = get_file_names(test_data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_files(directory, file_names):\n",
    "    texts = dict()\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file = directory + file_name + '.txt'\n",
    "\n",
    "        with open(file, 'r') as f:\n",
    "            texts[file_name] = f.read().replace('\\n', ' ')\n",
    "\n",
    "    return texts\n",
    "\n",
    "train_texts = read_files(train_data_directory, train_file_names)\n",
    "test_texts = read_files(test_data_directory, test_file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join\n",
    "\n",
    "def pkl_dump(path, file_name, object_):\n",
    "    with open(join(path, file_name), 'wb') as pkl_file:\n",
    "        pickle.dump(object_, pkl_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def preprocessing(texts, path2store, purpose_of_data=\"train\"):\n",
    "    \"\"\" get sentences, tokens and part-of-speech from texts by using NLTK.\n",
    "        Then store them into pkl files.\n",
    "    \"\"\"\n",
    "\n",
    "    sents  = dict()\n",
    "    tokens = dict()\n",
    "    pos    = dict()\n",
    "\n",
    "    for file_name, file_content in texts.items():\n",
    "        tokens_   = []\n",
    "        tags      = []\n",
    "\n",
    "        sentences = nltk.sent_tokenize(file_content)\n",
    "        for sentence in sentences:\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "            tokens_.append(words)\n",
    "            tags.append(nltk.pos_tag(words))\n",
    "\n",
    "        sents[file_name] = sentences\n",
    "        tokens[file_name] = tokens_\n",
    "        pos[file_name] = tags\n",
    "        \n",
    "    pkl_dump(path2store, purpose_of_data+\"_texts.pkl\", texts)\n",
    "    pkl_dump(path2store, purpose_of_data+\"_sents.pkl\", sents)\n",
    "    pkl_dump(path2store, purpose_of_data+\"_tokens.pkl\", tokens)\n",
    "    pkl_dump(path2store, purpose_of_data+\"_pos.pkl\", pos)\n",
    "\n",
    "    return\n",
    "\n",
    "preprocessing(train_texts, OBJECTS, \"train\")\n",
    "preprocessing(test_texts, OBJECTS, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For training data: Get annotated entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree\n",
    "\n",
    "entities = {}\n",
    "for file_name in train_texts.keys():\n",
    "    file_path = train_data_directory + file_name + '.xml'\n",
    "    \n",
    "    root = xml.etree.ElementTree.parse(file_path).getroot()\n",
    "    annotations = root.findall('annotations')[0]\n",
    "\n",
    "    entities_ = [] # get span of every annoted entity\n",
    "    for entity in annotations.findall('entity'):\n",
    "        id = entity.find('id').text\n",
    "        typee = entity.find('type')\n",
    "\n",
    "        if typee.text in ['FamilyMember', 'Observation', 'LivingStatus']: # other options: Age, PHI\n",
    "            properties = entity.find('properties')\n",
    "            for spans in entity.findall('span')[0].text.split(';'):\n",
    "                span = list(map(int, spans.split(',')))\n",
    "                attributes = {}\n",
    "                \n",
    "                if typee.text == 'FamilyMember':\n",
    "                    attributes['SideOfFamily'] = properties.find('SideOfFamily').text\n",
    "                elif typee.text == 'Observation':\n",
    "                    attributes['Negation'] = properties.find('Negation').text\n",
    "                elif typee.text == 'LivingStatus':\n",
    "                    attributes['Alive'] = properties.find('Alive').text\n",
    "                    attributes['Healthy'] = properties.find('Healthy').text\n",
    "                    attributes['ID'] = properties.find('ID').text\n",
    "                \n",
    "                entities_.append((span[0], span[1],\n",
    "                                  typee.text,\n",
    "                                  train_texts[file_name][span[0]:span[1]],\n",
    "                                  attributes,\n",
    "                                  id))\n",
    "\n",
    "    entities_.sort(key=lambda tup: tup[1])\n",
    "    entities[file_name] = entities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save entities to disk\n",
    "pkl_dump(OBJECTS, \"train_entities.pkl\", entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entities stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_entities = 0\n",
    "living_status = []\n",
    "for file_name in train_texts.keys():\n",
    "    print('\\n', file_name)\n",
    "    # print(texts[file_name])\n",
    "    \n",
    "    for entity in entities[file_name]:\n",
    "        num_entities += 1\n",
    "        if entity[2] == 'LivingStatus':\n",
    "            living_status.append(entity[3])\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 2202 enties in all.\n"
     ]
    }
   ],
   "source": [
    "print(\"there are\", num_entities, \"enties in all.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'died ': 39,\n",
       "         'living': 5,\n",
       "         'healthy ': 10,\n",
       "         'alive': 7,\n",
       "         'died': 123,\n",
       "         'good general health': 11,\n",
       "         'healthy': 118,\n",
       "         'deceased': 3,\n",
       "         'otherwise healthy': 1,\n",
       "         'living and well': 7,\n",
       "         'passed away ': 17,\n",
       "         'stillborn': 1,\n",
       "         'passed away': 11,\n",
       "         'good health': 32,\n",
       "         'alive ': 4,\n",
       "         'well': 5,\n",
       "         'alive and well': 1,\n",
       "         'health': 2,\n",
       "         'good general health ': 1,\n",
       "         'doing well': 1,\n",
       "         'death': 1,\n",
       "         'living ': 1,\n",
       "         ' living and well': 3,\n",
       "         ' healthy': 2,\n",
       "         ' health': 1,\n",
       "         'ied ': 1,\n",
       "         'without problems': 1,\n",
       "         'dead': 2,\n",
       "         'living and well ': 1,\n",
       "         'generally healthy': 1,\n",
       "         'health ': 1,\n",
       "         ' alive': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(living_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
