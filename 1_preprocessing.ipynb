{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation errors hava found:\n",
    "\n",
    "- doc_50: full anned as FM\n",
    "- doc_169: first char missing at idx 10\n",
    "- doc_59: 3 lost last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "directory  = './Data/bioc-FH-training/'\n",
    "file_list  = listdir(directory)\n",
    "file_names = set([file.split('.')[0] for file in file_list if file != '' and file.endswith('txt')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "#import chardet # chardet.detect(binary_var)['encoding']\n",
    "\n",
    "texts = dict()\n",
    "\n",
    "for file_name in file_names:\n",
    "    file = directory + file_name + '.txt'\n",
    "    \n",
    "    with open(file, 'r') as f:\n",
    "        texts[file_name] = f.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "sents  = dict()\n",
    "tokens = dict()\n",
    "pos    = dict()\n",
    "\n",
    "for file_name in file_names:\n",
    "    tokens_   = []\n",
    "    tags      = []\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(texts[file_name])\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        tokens_.append(words)\n",
    "        tags.append(nltk.pos_tag(words))\n",
    "\n",
    "    sents[file_name] = sentences\n",
    "    tokens[file_name] = tokens_\n",
    "    pos[file_name] = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get annotated entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree\n",
    "\n",
    "entities = {}\n",
    "for file_name in file_names:\n",
    "    file_path = directory + file_name + '.xml'\n",
    "    \n",
    "    root = xml.etree.ElementTree.parse(file_path).getroot()\n",
    "    annotations = root.findall('annotations')[0]\n",
    "\n",
    "    entities_ = [] # get span of every annoted entity\n",
    "    for entity in annotations.findall('entity'):\n",
    "        id = entity.find('id').text\n",
    "        typee = entity.find('type')\n",
    "\n",
    "        if typee.text in ['FamilyMember', 'Observation', 'LivingStatus']: # other options: Age, PHI\n",
    "            properties = entity.find('properties')\n",
    "            for spans in entity.findall('span')[0].text.split(';'):\n",
    "                span = list(map(int, spans.split(',')))\n",
    "                attributes = {}\n",
    "                \n",
    "                if typee.text == 'FamilyMember':\n",
    "                    attributes['SideOfFamily'] = properties.find('SideOfFamily').text\n",
    "                elif typee.text == 'Observation':\n",
    "                    attributes['Negation'] = properties.find('Negation').text\n",
    "                elif typee.text == 'LivingStatus':\n",
    "                    attributes['Alive'] = properties.find('Alive').text\n",
    "                    attributes['Healthy'] = properties.find('Healthy').text\n",
    "                    attributes['ID'] = properties.find('ID').text\n",
    "                \n",
    "                entities_.append((span[0], span[1],\n",
    "                                  typee.text,\n",
    "                                  texts[file_name][span[0]:span[1]],\n",
    "                                  attributes,\n",
    "                                  id))\n",
    "\n",
    "    entities_.sort(key=lambda tup: tup[1])\n",
    "    entities[file_name] = entities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_entities = 0\n",
    "living_status = []\n",
    "for file_name in file_names:\n",
    "    print('\\n', file_name)\n",
    "#     print(texts[file_name])\n",
    "    for entity in entities[file_name]:\n",
    "        num_entities += 1\n",
    "        if entity[2] == 'LivingStatus':\n",
    "            living_status.append(entity[3])\n",
    "        print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'died': 123,\n",
       "         'passed away': 11,\n",
       "         'alive': 7,\n",
       "         'well': 5,\n",
       "         'healthy': 118,\n",
       "         'good health': 32,\n",
       "         'health': 2,\n",
       "         'died ': 39,\n",
       "         'ied ': 1,\n",
       "         'living': 5,\n",
       "         'healthy ': 10,\n",
       "         'good general health': 11,\n",
       "         'passed away ': 17,\n",
       "         'living and well': 7,\n",
       "         ' living and well': 3,\n",
       "         ' healthy': 2,\n",
       "         'alive ': 4,\n",
       "         'deceased': 3,\n",
       "         ' alive': 1,\n",
       "         'dead': 2,\n",
       "         'alive and well': 1,\n",
       "         ' health': 1,\n",
       "         'doing well': 1,\n",
       "         'generally healthy': 1,\n",
       "         'health ': 1,\n",
       "         'living ': 1,\n",
       "         'without problems': 1,\n",
       "         'good general health ': 1,\n",
       "         'otherwise healthy': 1,\n",
       "         'stillborn': 1,\n",
       "         'death': 1,\n",
       "         'living and well ': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(living_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 2202 enties in all.\n"
     ]
    }
   ],
   "source": [
    "print(\"there are\", num_entities, \"enties in all.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store to pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join\n",
    "\n",
    "OBJECTS = './objects/'\n",
    "with open(join(OBJECTS, \"texts.pkl\"), 'wb') as pkl_file:\n",
    "    pickle.dump(texts, pkl_file)\n",
    "with open(join(OBJECTS, \"sents.pkl\"), 'wb') as pkl_file:\n",
    "    pickle.dump(sents, pkl_file)\n",
    "with open(join(OBJECTS, \"tokens.pkl\"), 'wb') as pkl_file:\n",
    "    pickle.dump(tokens, pkl_file)\n",
    "with open(join(OBJECTS, \"pos.pkl\"), 'wb') as pkl_file:\n",
    "    pickle.dump(pos, pkl_file)\n",
    "with open(join(OBJECTS, \"entities.pkl\"), 'wb') as pkl_file:\n",
    "    pickle.dump(entities, pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
